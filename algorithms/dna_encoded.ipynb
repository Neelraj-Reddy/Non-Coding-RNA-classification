{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9u6zsVxVE2g5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "CqbP2azpE2g9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Define CNN model\n",
        "# def create_cnn_model(input_shape, num_classes):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(32, activation='relu', input_shape=input_shape))\n",
        "#     model.add(MaxPooling1D(pool_size=3))\n",
        "#     # model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
        "#     # model.add(MaxPooling1D(pool_size=2))\n",
        "#     model.add(LSTM(64))\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(64, activation='relu'))\n",
        "#     model.add(Dense(num_classes, activation='softmax'))\n",
        "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "# def create_cnn_model(input_shape, num_classes):\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "#     model.add(MaxPooling1D(pool_size=2))\n",
        "#     model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
        "#     model.add(MaxPooling1D(pool_size=2))\n",
        "#     model.add(LSTM(64))  # Adding LSTM layer\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(64, activation='relu'))\n",
        "#     model.add(Dense(num_classes, activation='softmax'))\n",
        "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "\n",
        "# def create_nn_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Dense(128, activation='relu', input_shape=input_shape),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         Dropout(0.5),\n",
        "#         Flatten(),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "\n",
        "#     model.compile(optimizer='adam',\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "    \n",
        "#     return model\n",
        "\n",
        "\n",
        "# def create_improved_nn_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Dense(256, activation='relu', input_shape=input_shape),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.5),\n",
        "#         Flatten(),\n",
        "#         Dense(32, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "\n",
        "#     model.compile(optimizer='adam',\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "    \n",
        "#     return model\n",
        "\n",
        "def create_nn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        BatchNormalization(),\n",
        "        Conv1D(64, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        BatchNormalization(),\n",
        "        # LSTM(64, return_sequences=True),\n",
        "        LSTM(32),\n",
        "        BatchNormalization(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hdXx_KEvE2g-"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv(r'E:\\4th sem\\IOBS-2\\project\\Non Coding RNA classification\\Cleaned_data\\EIIP\\0-100_features.csv')\n",
        "y = pd.read_csv(r\"E:\\4th sem\\IOBS-2\\project\\Non Coding RNA classification\\Cleaned_data\\EIIP\\0-100_class.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyw9pQjqE2g_",
        "outputId": "2066efb2-5570-41d2-e3a3-fd0f5c92c7fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\neelr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and transform labels\n",
        "encoded_labels = label_encoder.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjM5PEF3E2hA",
        "outputId": "6653b988-dca3-4dbc-e2b6-60c3cf1e9b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1452 1452\n"
          ]
        }
      ],
      "source": [
        "print(len(X),len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU992hyWIuky",
        "outputId": "e27e137a-b8eb-43ee-e05c-9a01fb28b549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set shapes: (1306, 100) (1306,)\n",
            "Test set shapes: (73, 100) (73,)\n",
            "Validation set shapes: (73, 100) (73,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train (80%) and combined test/validation (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, encoded_labels, test_size=0.1, shuffle=True, random_state=42)\n",
        "\n",
        "# Split combined test/validation into test (10%) and validation (10%)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.50, shuffle=True, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting sets\n",
        "print(\"Train set shapes:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shapes:\", X_test.shape, y_test.shape)\n",
        "print(\"Validation set shapes:\", X_val.shape, y_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeoUu_5fE2hA",
        "outputId": "dc27a3bf-773b-45ca-f7fe-64b80169c6ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_14 (Conv1D)          (None, 98, 32)            128       \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPooli  (None, 49, 32)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 49, 32)            128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 47, 64)            6208      \n",
            "                                                                 \n",
            " max_pooling1d_20 (MaxPooli  (None, 23, 64)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 23, 64)            256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 32)                12416     \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 12)                780       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22156 (86.55 KB)\n",
            "Trainable params: 21900 (85.55 KB)\n",
            "Non-trainable params: 256 (1.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data\n",
        "max_sequence_length = 100  # Example maximum sequence length\n",
        "\n",
        "\n",
        "# Reshape data for CNN input\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Create CNN model\n",
        "model = create_nn_model(input_shape, num_classes)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "41/41 [==============================] - 1s 21ms/step - loss: 0.8330 - accuracy: 0.7412 - val_loss: 1.3457 - val_accuracy: 0.5205\n",
            "Epoch 2/15\n",
            "41/41 [==============================] - 1s 21ms/step - loss: 0.7964 - accuracy: 0.7473 - val_loss: 1.4526 - val_accuracy: 0.5342\n",
            "Epoch 3/15\n",
            "41/41 [==============================] - 1s 20ms/step - loss: 0.7642 - accuracy: 0.7481 - val_loss: 1.3205 - val_accuracy: 0.6027\n",
            "Epoch 4/15\n",
            "41/41 [==============================] - 1s 22ms/step - loss: 0.7361 - accuracy: 0.7565 - val_loss: 1.1186 - val_accuracy: 0.6712\n",
            "Epoch 5/15\n",
            "41/41 [==============================] - 1s 22ms/step - loss: 0.7008 - accuracy: 0.7772 - val_loss: 1.4531 - val_accuracy: 0.4658\n",
            "Epoch 6/15\n",
            "41/41 [==============================] - 1s 20ms/step - loss: 0.6541 - accuracy: 0.7887 - val_loss: 1.2647 - val_accuracy: 0.5753\n",
            "Epoch 7/15\n",
            "41/41 [==============================] - 1s 22ms/step - loss: 0.6843 - accuracy: 0.7695 - val_loss: 1.2073 - val_accuracy: 0.6575\n",
            "Epoch 8/15\n",
            "41/41 [==============================] - 1s 21ms/step - loss: 0.6364 - accuracy: 0.8070 - val_loss: 1.8413 - val_accuracy: 0.5205\n",
            "Epoch 9/15\n",
            "41/41 [==============================] - 1s 21ms/step - loss: 0.6682 - accuracy: 0.7917 - val_loss: 1.2268 - val_accuracy: 0.6849\n",
            "Epoch 10/15\n",
            "41/41 [==============================] - 1s 22ms/step - loss: 0.6303 - accuracy: 0.7940 - val_loss: 1.3103 - val_accuracy: 0.5753\n",
            "Epoch 11/15\n",
            "41/41 [==============================] - 1s 20ms/step - loss: 0.5593 - accuracy: 0.8178 - val_loss: 1.2463 - val_accuracy: 0.6164\n",
            "Epoch 12/15\n",
            "41/41 [==============================] - 1s 22ms/step - loss: 0.5512 - accuracy: 0.8338 - val_loss: 1.7578 - val_accuracy: 0.5342\n",
            "Epoch 13/15\n",
            "41/41 [==============================] - 1s 24ms/step - loss: 0.5098 - accuracy: 0.8277 - val_loss: 1.3370 - val_accuracy: 0.6301\n",
            "Epoch 14/15\n",
            "41/41 [==============================] - 1s 21ms/step - loss: 0.4755 - accuracy: 0.8308 - val_loss: 1.2480 - val_accuracy: 0.6575\n",
            "Epoch 15/15\n",
            "41/41 [==============================] - 1s 22ms/step - loss: 0.4943 - accuracy: 0.8346 - val_loss: 1.3661 - val_accuracy: 0.6301\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 1.6945 - accuracy: 0.5342\n",
            "Test Accuracy: 0.534246563911438\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train CNN model\n",
        "# model.fit(X, encoded_labels, epochs=200)\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=15, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print test accuracy\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NARzL6XpE2hA",
        "outputId": "8d75f049-69d1-44e6-c3dc-ad51ae18c7ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\neelr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "X_200 = pd.read_csv(r'E:\\4th sem\\IOBS-2\\project\\Non Coding RNA classification\\Cleaned_data\\100-200_features.csv')\n",
        "y_200 = pd.read_csv(r\"E:\\4th sem\\IOBS-2\\project\\Non Coding RNA classification\\Cleaned_data\\100-200_class.csv\")\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder and transform labels\n",
        "encoded_labels_200 = label_encoder.fit_transform(y_200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5wL3SWGE2hB"
      },
      "outputs": [],
      "source": [
        "encoded_labels_200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5EkSlXXE2hB",
        "outputId": "3b4c06c7-9229-4f9f-c474-60bdcf0f0c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "94/94 [==============================] - 2s 7ms/step - loss: 2.0225 - accuracy: 0.3435\n",
            "Epoch 2/30\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 1.5691 - accuracy: 0.4851\n",
            "Epoch 3/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 1.3541 - accuracy: 0.5733\n",
            "Epoch 4/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 1.1960 - accuracy: 0.6215\n",
            "Epoch 5/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 1.0711 - accuracy: 0.6579\n",
            "Epoch 6/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.9670 - accuracy: 0.6943\n",
            "Epoch 7/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.8875 - accuracy: 0.7207\n",
            "Epoch 8/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.8085 - accuracy: 0.7421\n",
            "Epoch 9/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.7309 - accuracy: 0.7655\n",
            "Epoch 10/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.6782 - accuracy: 0.7868\n",
            "Epoch 11/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.6221 - accuracy: 0.8032\n",
            "Epoch 12/30\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.5579 - accuracy: 0.8233\n",
            "Epoch 13/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.5132 - accuracy: 0.8363\n",
            "Epoch 14/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.4431 - accuracy: 0.8577\n",
            "Epoch 15/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.4147 - accuracy: 0.8674\n",
            "Epoch 16/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.3572 - accuracy: 0.8911\n",
            "Epoch 17/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.3246 - accuracy: 0.9048\n",
            "Epoch 18/30\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.2845 - accuracy: 0.9131\n",
            "Epoch 19/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.2488 - accuracy: 0.9282\n",
            "Epoch 20/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.2141 - accuracy: 0.9489\n",
            "Epoch 21/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1947 - accuracy: 0.9429\n",
            "Epoch 22/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1604 - accuracy: 0.9599\n",
            "Epoch 23/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1346 - accuracy: 0.9676\n",
            "Epoch 24/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.1186 - accuracy: 0.9746\n",
            "Epoch 25/30\n",
            "94/94 [==============================] - 1s 7ms/step - loss: 0.0979 - accuracy: 0.9826\n",
            "Epoch 26/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0788 - accuracy: 0.9883\n",
            "Epoch 27/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0653 - accuracy: 0.9930\n",
            "Epoch 28/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0441 - accuracy: 0.9973\n",
            "Epoch 29/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0414 - accuracy: 0.9983\n",
            "Epoch 30/30\n",
            "94/94 [==============================] - 1s 6ms/step - loss: 0.0346 - accuracy: 0.9990\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1a2ead03430>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Now, you want to train the model for a new input length (200) without changing the architecture\n",
        "# Assuming you have new data X_200 and encoded_labels_200 for the new input length\n",
        "\n",
        "# Create a new CNN model with the same architecture as the previous model\n",
        "# No need to compile the model as we will be loading the weights\n",
        "model = create_cnn_model((200, 1), 13)\n",
        "\n",
        "# Load weights from the previously trained model\n",
        "# new_model.set_weights(model.get_weights())\n",
        "\n",
        "# Train the new model with the new data for 200 input length\n",
        "model.fit(X_200, encoded_labels_200, epochs=30)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
