{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":6874504,"sourceType":"datasetVersion","datasetId":3950318}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## DNA Sequencing With Machine Learning\n\nIn this notebook, I will apply a classification model that can predict a gene's function based on the DNA sequence of the coding sequence alone.","metadata":{"id":"6UMcBpCd_7jz"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"DC5Qjc-A_7j1","execution":{"iopub.status.busy":"2024-03-27T07:39:47.211492Z","iopub.execute_input":"2024-03-27T07:39:47.211929Z","iopub.status.idle":"2024-03-27T07:39:48.455582Z","shell.execute_reply.started":"2024-03-27T07:39:47.211868Z","shell.execute_reply":"2024-03-27T07:39:48.454361Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming your CSV file is named 'data.csv'\n# Adjust the file path accordingly\nfile_path = '/kaggle/input/covid-deeppredictor/TrainingData/Trainingdata.csv'\n\n# Reading CSV into DataFrame\ndf = pd.read_csv(file_path, usecols=[1, 3])\ndf.columns = ['class', 'sequence']\n# Displaying the DataFrame\nprint(df)","metadata":{"id":"-GLF4mqm_7j2","execution":{"iopub.status.busy":"2024-03-27T07:39:48.458170Z","iopub.execute_input":"2024-03-27T07:39:48.458779Z","iopub.status.idle":"2024-03-27T07:39:49.156736Z","shell.execute_reply.started":"2024-03-27T07:39:48.458734Z","shell.execute_reply":"2024-03-27T07:39:49.155303Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"      class                                           sequence\n0         1  GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTA...\n1         1  CATTCAGTACGGTCGTAGCGGTATAACACTGGGAGTACTCGTGCCA...\n2         1  CACGCGCGGGCAAGTCAATGTGCACTCTTTCCGAACAACTTGATTA...\n3         1  ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...\n4         1  ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...\n...     ...                                                ...\n1495      6  AGTATGGAAAGAATAAAAGAACTACGGACCCTGATGTCGCAGTCTC...\n1496      6  AAAGCAGGCAAACCATTTGAATGGATGTCAATCCGACTCTACTTTT...\n1497      6  AGTATGGAAAGAATAAAAGAACTACGGACCCTGATGTCGCAGTCTC...\n1498      6  ATTTGAATGGATGTCAATCCGACTCTACTTTTCCTAAAGGTTCCAG...\n1499      6  AGTATGGAAAGAATAAAAGAACTACGGACCCTGATGTCGCAGTCTC...\n\n[1500 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### We have some data for human DNA sequence coding regions and a class label.  We also have data for Chimpanzee and a more divergent species, the dog.","metadata":{"id":"YP-a1WVp_7j3"}},{"cell_type":"code","source":"file_pathlol = '/kaggle/input/covid-deeppredictor/TestData/TestData/Testdata-1.csv'\n\n# Reading CSV into DataFrame\ntest = pd.read_csv(file_pathlol, usecols=[1, 2])\ntest.columns = ['class', 'sequence']\n# Displaying the DataFrame\n\n","metadata":{"id":"6iXU7TsU_7j3","execution":{"iopub.status.busy":"2024-03-27T07:39:49.158500Z","iopub.execute_input":"2024-03-27T07:39:49.158940Z","iopub.status.idle":"2024-03-27T07:39:50.744317Z","shell.execute_reply.started":"2024-03-27T07:39:49.158891Z","shell.execute_reply":"2024-03-27T07:39:50.742971Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Here are the definitions for each of the 7 classes and how many there are in the human training data.  They are gene sequence function groups.","metadata":{"id":"wNCkbr_n_7j4"}},{"cell_type":"code","source":"","metadata":{"id":"hNWvZ_gDOAqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"i2jNoohq_7j4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0796cf3f-a5f7-47c6-9c0a-ee1076ed48f4","execution":{"iopub.status.busy":"2024-03-27T07:39:50.749752Z","iopub.execute_input":"2024-03-27T07:39:50.750151Z","iopub.status.idle":"2024-03-27T07:39:50.773703Z","shell.execute_reply.started":"2024-03-27T07:39:50.750118Z","shell.execute_reply":"2024-03-27T07:39:50.772462Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   class     1500 non-null   int64 \n 1   sequence  1500 non-null   object\ndtypes: int64(1), object(1)\nmemory usage: 23.6+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{"id":"pQ1Y8-3b_7j5"}},{"cell_type":"markdown","source":"### Let's define a function to collect all possible overlapping k-mers of a specified length from any sequence string. We will basically apply the k-mers to the complete sequences.","metadata":{"id":"7s4Q7MV2_7j5"}},{"cell_type":"code","source":"# function to convert sequence strings into k-mer words, default size = 6 (hexamer words)\ndef getKmers(sequence, size=6):\n    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]","metadata":{"id":"IcBoJDcS_7j6","execution":{"iopub.status.busy":"2024-03-27T07:39:50.775457Z","iopub.execute_input":"2024-03-27T07:39:50.775943Z","iopub.status.idle":"2024-03-27T07:39:50.782062Z","shell.execute_reply.started":"2024-03-27T07:39:50.775898Z","shell.execute_reply":"2024-03-27T07:39:50.780742Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"human_data = pd.concat([df, test], ignore_index=True)","metadata":{"id":"3z0mD9noFL5e","execution":{"iopub.status.busy":"2024-03-27T07:39:50.784059Z","iopub.execute_input":"2024-03-27T07:39:50.784487Z","iopub.status.idle":"2024-03-27T07:39:50.796371Z","shell.execute_reply.started":"2024-03-27T07:39:50.784449Z","shell.execute_reply":"2024-03-27T07:39:50.795361Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"human_data","metadata":{"id":"4F2uNXSYs1XV","colab":{"base_uri":"https://localhost:8080/","height":423},"outputId":"9377264f-c106-4fe9-cb36-6281834f9fd7","execution":{"iopub.status.busy":"2024-03-27T07:39:50.797794Z","iopub.execute_input":"2024-03-27T07:39:50.798537Z","iopub.status.idle":"2024-03-27T07:39:50.820208Z","shell.execute_reply.started":"2024-03-27T07:39:50.798504Z","shell.execute_reply":"2024-03-27T07:39:50.818875Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      class                                           sequence\n0         1  GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTA...\n1         1  CATTCAGTACGGTCGTAGCGGTATAACACTGGGAGTACTCGTGCCA...\n2         1  CACGCGCGGGCAAGTCAATGTGCACTCTTTCCGAACAACTTGATTA...\n3         1  ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...\n4         1  ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...\n...     ...                                                ...\n4638      6  TCAAATATATTCAATATGGAGAGAATAAAAGAGCTGAGAGATCTAA...\n4639      6  TCAAATATATTCAATATGGAGAGAATAAAAGAGCTGAGAGATCTAA...\n4640      6  CAAACCATTTGAATGGATGTCAATCCGACTCTACTTTTCTTAAAAA...\n4641      6  TCAAATATATTCAATATGGAGAGAATAAAAGAGCTGAGAGATCTAA...\n4642      6  CAAACCATTTGAATGGATGTCAATCCGACTCTACTTTTCTTAAAAA...\n\n[4643 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>GATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CATTCAGTACGGTCGTAGCGGTATAACACTGGGAGTACTCGTGCCA...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>CACGCGCGGGCAAGTCAATGTGCACTCTTTCCGAACAACTTGATTA...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>ATATTAGGTTTTTACCTACCCAGGAAAAGCCAACCAACCTCGATCT...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4638</th>\n      <td>6</td>\n      <td>TCAAATATATTCAATATGGAGAGAATAAAAGAGCTGAGAGATCTAA...</td>\n    </tr>\n    <tr>\n      <th>4639</th>\n      <td>6</td>\n      <td>TCAAATATATTCAATATGGAGAGAATAAAAGAGCTGAGAGATCTAA...</td>\n    </tr>\n    <tr>\n      <th>4640</th>\n      <td>6</td>\n      <td>CAAACCATTTGAATGGATGTCAATCCGACTCTACTTTTCTTAAAAA...</td>\n    </tr>\n    <tr>\n      <th>4641</th>\n      <td>6</td>\n      <td>TCAAATATATTCAATATGGAGAGAATAAAAGAGCTGAGAGATCTAA...</td>\n    </tr>\n    <tr>\n      <th>4642</th>\n      <td>6</td>\n      <td>CAAACCATTTGAATGGATGTCAATCCGACTCTACTTTTCTTAAAAA...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4643 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df = pd.concat([df, tes], )","metadata":{"id":"u3F-XwEF1Jct","execution":{"iopub.status.busy":"2024-03-27T07:39:50.821779Z","iopub.execute_input":"2024-03-27T07:39:50.822202Z","iopub.status.idle":"2024-03-27T07:39:50.828441Z","shell.execute_reply.started":"2024-03-27T07:39:50.822169Z","shell.execute_reply":"2024-03-27T07:39:50.827090Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Now we can convert our training data sequences into short overlapping  k-mers of legth 6.  Lets do that for each species of data we have using our getKmers function.","metadata":{"id":"VxK43poK_7j6"}},{"cell_type":"code","source":"human_data['words'] = human_data.apply(lambda x: getKmers(x['sequence']), axis=1)\nhuman_data = human_data.drop('sequence', axis=1)\n","metadata":{"id":"J-S9fivr_7j6","execution":{"iopub.status.busy":"2024-03-27T07:39:50.830332Z","iopub.execute_input":"2024-03-27T07:39:50.830940Z","iopub.status.idle":"2024-03-27T07:40:30.217720Z","shell.execute_reply.started":"2024-03-27T07:39:50.830899Z","shell.execute_reply":"2024-03-27T07:40:30.216297Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# tes['words'] = tes.apply(lambda x: getKmers(x['sequence']), axis=1)\n# tes = tes.drop('sequence', axis=1)","metadata":{"id":"IpM3rzJBsuxD","execution":{"iopub.status.busy":"2024-03-27T07:40:30.222602Z","iopub.execute_input":"2024-03-27T07:40:30.223062Z","iopub.status.idle":"2024-03-27T07:40:30.228176Z","shell.execute_reply.started":"2024-03-27T07:40:30.223025Z","shell.execute_reply":"2024-03-27T07:40:30.226503Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Now, our coding sequence data is changed to lowercase, split up into all possible k-mer words of length 6 and ready for the next step.  Let's take a look.","metadata":{"id":"JoxQ0k_b_7j7"}},{"cell_type":"code","source":"# # Assuming 'sequence' is the name of the column containing DNA sequences\n# # Assuming df is your DataFrame with the 'sequence' column\n\n# import pandas as pd\n# import re\n\n# # Assuming df is your DataFrame and 'sequence' is the column containing DNA sequences\n\n# # Define a function to check if a sequence contains only DNA nucleotides\n# def is_valid_sequence(sequence):\n#     pattern = re.compile(r'[^ATGC]', re.IGNORECASE)  # Regular expression to match non-DNA nucleotides\n#     return not bool(pattern.search(sequence))\n\n# # Filter out rows with invalid sequences\n# df = df[df['sequence'].apply(is_valid_sequence)]\n\n# # Now df contains only rows with valid DNA sequences\n","metadata":{"id":"MBeZmt6G_7j7","execution":{"iopub.status.busy":"2024-03-27T05:50:39.320915Z","iopub.status.idle":"2024-03-27T05:50:39.321344Z","shell.execute_reply.started":"2024-03-27T05:50:39.321112Z","shell.execute_reply":"2024-03-27T05:50:39.321128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"yEfLhqnZvZDT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Since we are going to use scikit-learn natural language processing tools to do the k-mer counting, we need to now convert the lists of k-mers for each gene into string sentences of words that the count vectorizer can use.  We can also make a y variable to hold the class labels.  Let's do that now.","metadata":{"id":"ru6Azh_V_7j8"}},{"cell_type":"code","source":"human_texts = list(human_data['words'])\nfor item in range(len(human_texts)):\n    human_texts[item] = ' '.join(human_texts[item])\ny_data = human_data.iloc[:, 0].values","metadata":{"id":"tC03xon3_7j8","execution":{"iopub.status.busy":"2024-03-27T07:40:30.230570Z","iopub.execute_input":"2024-03-27T07:40:30.231113Z","iopub.status.idle":"2024-03-27T07:40:35.668107Z","shell.execute_reply.started":"2024-03-27T07:40:30.231063Z","shell.execute_reply":"2024-03-27T07:40:35.666578Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# lol = list(tes['words'])\n# for item in range(len(lol)):\n#   lol[item] = ' '.join(lol[item])\n# y_tes = tes.iloc[:, 0].values","metadata":{"id":"la1kHeyxsdc5","execution":{"iopub.status.busy":"2024-03-27T05:50:39.324763Z","iopub.status.idle":"2024-03-27T05:50:39.325257Z","shell.execute_reply.started":"2024-03-27T05:50:39.324997Z","shell.execute_reply":"2024-03-27T05:50:39.325017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZCUQ6Jk_7j8","outputId":"1b2e67b6-1f58-4413-ce72-9bea5b7ecbd4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(y_data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMKW8tCJ_7j9","outputId":"2ab9c009-b8f2-497b-803a-cfc0bdb5a9a3","execution":{"iopub.status.busy":"2024-03-27T05:51:28.507058Z","iopub.execute_input":"2024-03-27T05:51:28.507455Z","iopub.status.idle":"2024-03-27T05:51:28.515776Z","shell.execute_reply.started":"2024-03-27T05:51:28.507425Z","shell.execute_reply":"2024-03-27T05:51:28.514586Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([1, 2, 3, 4, 5, 6])"},"metadata":{}}]},{"cell_type":"markdown","source":"## We will perform the same steps for chimpanzee and dog","metadata":{"id":"LrXEn2Nn_7j9"}},{"cell_type":"code","source":"# X=human_texts\nX=human_data['words']","metadata":{"id":"UMnXzBQcAPx1","execution":{"iopub.status.busy":"2024-03-27T07:43:04.345111Z","iopub.execute_input":"2024-03-27T07:43:04.345517Z","iopub.status.idle":"2024-03-27T07:43:04.350847Z","shell.execute_reply.started":"2024-03-27T07:43:04.345487Z","shell.execute_reply":"2024-03-27T07:43:04.349242Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(X[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfMlhvc-J3n2","outputId":"30f58d2e-c4f9-4d85-ca11-f71c688e30fc","execution":{"iopub.status.busy":"2024-03-27T05:50:39.332828Z","iopub.status.idle":"2024-03-27T05:50:39.333170Z","shell.execute_reply.started":"2024-03-27T05:50:39.332999Z","shell.execute_reply":"2024-03-27T05:50:39.333013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n# from tensorflow.keras.optimizers import Adam\n\n# all_words = [word for text in X for word in text.split()]\n\n# # Determine the vocabulary size\n# vocab_size = len(set(all_words))\n# from tensorflow.keras.utils import to_categorical\n\n# # Assuming y_data contains the categorical labels\n# # Perform one-hot encoding\n# y_data_encoded = to_categorical(y_data, num_classes=num_classes)\n# # Determine the maximum sequence length\n# max_length = max(len(text.split()) for text in X)\n# num_classes = 82\n# # Model Architecture\n# model = Sequential([\n#     Embedding(vocab_size, 100, input_length=max_length),\n#     Conv1D(128, 5, activation='relu'),\n#     MaxPooling1D(),\n#     Flatten(),\n#     Dense(64, activation='relu'),\n#     Dense(82, activation='softmax')\n# ])\n\n# # Model Compilation\n# model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n# X_train_padded = pad_sequences(X_train, maxlen=max_length)\n# X_test_padded = pad_sequences(X_test, maxlen=max_length)\n# # Model Training\n# model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_val, y_val))\n\n# # Model Evaluation\n# loss, accuracy = model.evaluate(X_test_padded, y_test)\n# print(f'Test Accuracy: {accuracy}')\n\n# # Model Deployment (optional)\n# # Use the trained model for predictions on new data\n","metadata":{"id":"5ycBYYW6_7j9","execution":{"iopub.status.busy":"2024-03-27T05:50:39.334606Z","iopub.status.idle":"2024-03-27T05:50:39.335091Z","shell.execute_reply.started":"2024-03-27T05:50:39.334841Z","shell.execute_reply":"2024-03-27T05:50:39.334862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"ilG2pXAK_7j9"}},{"cell_type":"code","source":"from gensim.models import FastText\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n# Assuming X contains sequences split into k-mers and y_data contains labels\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y_data)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.4, random_state=42)\n\n# Train k-mer2vec model using FastText\nkmer2vec_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4)\n\n# Function to convert sequences to vectors using k-mer2vec model\ndef sequence_to_vector(sequence, model):\n    vectors = [model.wv[word] for word in sequence if word in model.wv]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n\n# Convert sequences to vectors\nX_train_vectors = np.array([sequence_to_vector(sequence, kmer2vec_model) for sequence in X_train])\nX_test_vectors = np.array([sequence_to_vector(sequence, kmer2vec_model) for sequence in X_test])\n\n# Now you can use X_train_vectors and X_test_vectors as input to your machine learning model\n","metadata":{"id":"qMWDD1jvJIgM","execution":{"iopub.status.busy":"2024-03-27T07:43:07.618210Z","iopub.execute_input":"2024-03-27T07:43:07.618634Z","iopub.status.idle":"2024-03-27T07:59:53.637416Z","shell.execute_reply.started":"2024-03-27T07:43:07.618603Z","shell.execute_reply":"2024-03-27T07:59:53.635984Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Creating the Bag of Words model using CountVectorizer()\n# This is equivalent to k-mer counting\n# The n-gram size of 4 was previously determined by testing\n# from sklearn.feature_extraction.text import CountVectorizer\n# cv = CountVectorizer(ngram_range=(4,4))\n# X = cv.fit_transform(human_texts)\n# X_test= cv.fit_transform(lol)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(ngram_range=(1, 1))\n\n\nX_train_= tfidf.fit_transform(human_texts)\n\n# X_test= tfidf.fit_transform(lol)\n\n# X_chimp = cv.transform(chimp_texts)\n# X_dog = cv.transform(dog_texts)","metadata":{"id":"wQQCQWNb_7j-","execution":{"iopub.status.busy":"2024-03-27T05:50:39.338908Z","iopub.status.idle":"2024-03-27T05:50:39.339428Z","shell.execute_reply.started":"2024-03-27T05:50:39.339138Z","shell.execute_reply":"2024-03-27T05:50:39.339158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n\n# # Convert sparse matrix X to a dense numpy array\n# X_dense = X.toarray()\n# label_encoder = LabelEncoder()\n\n# # Fit and transform the 'Species' column\n# y_data = label_encoder.fit_transform(y_data)\n# # Split the data into train, test, and validation sets\n# X_train, X_test, y_train, y_test = train_test_split(X_dense, y_data, test_size=0.2, random_state=42)\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)  # 0.25 * 0.8 = 0.2","metadata":{"id":"KANBI_J__7j-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train","metadata":{"id":"flSS85_87XP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# # Define CNN model\n# model = Sequential()\n\n# # Embedding layer (optional, depends on the size and nature of your text data)\n# #model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length))\n\n# # Convolutional layers\n# model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)))\n# model.add(GlobalMaxPooling1D())\n# num_classes = 82\n\n# # Dense layers\n# model.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(num_classes, activation='softmax'))  # num_classes is the number of output classes\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# # Reshape X_train, X_val, and X_test to match the input shape of the model\n# X_train_reshaped = np.expand_dims(X_train, axis=-1)\n# X_val_reshaped = np.expand_dims(X_val, axis=-1)\n# X_test_reshaped = np.expand_dims(X_test, axis=-1)\n\n# # Train the model\n# model.fit(X_train_reshaped, y_train, epochs=30, batch_size=32, validation_data=(X_val_reshaped, y_val))\n\n# # Evaluate the model on the test set\n# test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n# print(\"Test Accuracy:\", test_accuracy)\n","metadata":{"id":"GbzfHe19KJTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"LpdGFIcU7RDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If we have a look at class balance we can see we have relatively balanced dataset.","metadata":{"id":"eCSfic-k_7j-"}},{"cell_type":"code","source":"","metadata":{"id":"r5ZsHzuPJgUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"human_data['class'].value_counts().sort_index().plot.bar()","metadata":{"id":"AqbJkZpV_7j-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the human dataset into the training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train_,\n                                                    y_data,\n                                                    test_size = 0.20,\n                                                    random_state=42)\n# y_train=y_data\n# y_test=y_tes\n# X_test_ = X_test[:, :173952]","metadata":{"id":"inUBpYTt_7j_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"id":"-bGJn_nl_7j_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, GlobalMaxPooling1D, Bidirectional\n\n# Assuming you have y_train as an array of labels and X_train as your BoW matrix\n\n# Convert sparse BoW matrix to a dense array\nX_train_dense = X_train_vectors\n\n# Splitting the Data\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_dense, y_train, test_size=0.2, random_state=42)\n\n# Define the CNN Model\n# model = Sequential([\n#     Embedding(input_dim=X_train_dense.shape[1], output_dim=128),\n#     Conv1D(filters=64, kernel_size=5, activation='relu'),\n#     # MaxPooling1D(pool_size=5),\n#     # Conv1D(filters=128, kernel_size=5, activation='relu'),\n#     GlobalMaxPooling1D(),\n#     Dense(64, activation='relu'),\n#     Dense(len(np.unique(y_train)), activation='softmax')\n# ])\nmodel = Sequential([\n    Embedding(input_dim=X_train_dense.shape[1], output_dim=128),\n    Conv1D(filters=64, kernel_size=5, activation='relu'),\n    MaxPooling1D(pool_size=5),\n    Bidirectional(LSTM(units=64, return_sequences=True)),\n    Bidirectional(LSTM(units=32)),\n    Dense(64, activation='relu'),\n    Dense(6, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Training the Model\nmodel.fit(X_train_split, y_train_split, epochs=10, batch_size=32, validation_data=(X_val_split, y_val_split))\n\n# Model Evaluation\nloss, accuracy = model.evaluate(X_val_split, y_val_split)\nprint(f'Validation Accuracy: {accuracy}')\n","metadata":{"id":"kl0cbRrOJoCJ","execution":{"iopub.status.busy":"2024-03-27T06:35:12.373818Z","iopub.execute_input":"2024-03-27T06:35:12.374232Z","iopub.status.idle":"2024-03-27T06:35:35.771709Z","shell.execute_reply.started":"2024-03-27T06:35:12.374204Z","shell.execute_reply":"2024-03-27T06:35:35.770485Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.5255 - loss: 1.4390 - val_accuracy: 0.7201 - val_loss: 0.9894\nEpoch 2/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7416 - loss: 0.8490 - val_accuracy: 0.7201 - val_loss: 0.7738\nEpoch 3/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7723 - loss: 0.7238 - val_accuracy: 0.7954 - val_loss: 0.6744\nEpoch 4/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8051 - loss: 0.6543 - val_accuracy: 0.7954 - val_loss: 0.6592\nEpoch 5/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7967 - loss: 0.6621 - val_accuracy: 0.7968 - val_loss: 0.6515\nEpoch 6/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7995 - loss: 0.6574 - val_accuracy: 0.7968 - val_loss: 0.6495\nEpoch 7/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7979 - loss: 0.6612 - val_accuracy: 0.7968 - val_loss: 0.6546\nEpoch 8/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8168 - loss: 0.6157 - val_accuracy: 0.7968 - val_loss: 0.6535\nEpoch 9/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8131 - loss: 0.6167 - val_accuracy: 0.7968 - val_loss: 0.6599\nEpoch 10/10\n\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8130 - loss: 0.6279 - val_accuracy: 0.7968 - val_loss: 0.6567\n\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8164 - loss: 0.6149\nValidation Accuracy: 0.7967698574066162\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, GlobalMaxPooling1D, LSTM\n\n# Assuming you have y_train as an array of labels and X_train as your BoW matrix\n# Assuming you've imported X_train and y_train\n\n# Convert sparse BoW matrix to a dense array\nX_train_dense = X_train_vectors\n\n# Splitting the Data\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_dense, y_train, test_size=0.2, random_state=42)\n\n# Define the CNN Model\nmax_words = X_train_dense.shape[1]  # Assuming each document in X_train has the same number of words\nnum_classes = 6\n\nmodel = Sequential([\n    Embedding(input_dim=max_words, output_dim=128, input_length=max_words),\n    # Conv1D(filters=64, kernel_size=5, activation='relu'),\n    # MaxPooling1D(pool_size=10),\n    # Conv1D(filters=64, kernel_size=5, activation='relu'),\n    # GlobalMaxPooling1D(),\n    # Dense(64, activation='relu'),\n    # Dense(num_classes, activation='softmax')\n    LSTM(units=64, return_sequences=True),\n    LSTM(units=32),\n    Dense(64, activation='relu'),\n    Dense(num_classes, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Training the Model\nmodel.fit(X_train_split, y_train_split, epochs=10, batch_size=32, validation_data=(X_val_split, y_val_split))\n\n# Model Evaluation\nloss, accuracy = model.evaluate(X_val_split, y_val_split)\nprint(f'Validation Accuracy: {accuracy}')\n","metadata":{"id":"lI8i68xa-IYf","execution":{"iopub.status.busy":"2024-03-27T06:22:35.963710Z","iopub.execute_input":"2024-03-27T06:22:35.964153Z","iopub.status.idle":"2024-03-27T06:22:36.085053Z","shell.execute_reply.started":"2024-03-27T06:22:35.964115Z","shell.execute_reply":"2024-03-27T06:22:36.083381Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m max_words \u001b[38;5;241m=\u001b[39m X_train_dense\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming each document in X_train has the same number of words\u001b[39;00m\n\u001b[1;32m     18\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_words\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Conv1D(filters=64, kernel_size=5, activation='relu'),\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# MaxPooling1D(pool_size=10),\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Conv1D(filters=64, kernel_size=5, activation='relu'),\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# GlobalMaxPooling1D(),\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Dense(64, activation='relu'),\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Dense(num_classes, activation='softmax')\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     29\u001b[0m     LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m),\n\u001b[1;32m     30\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     31\u001b[0m     Dense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m ])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:265\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_policy \u001b[38;5;241m=\u001b[39m dtype_policies\u001b[38;5;241m.\u001b[39mget(dtype)\n","\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'input_length': 100}"],"ename":"ValueError","evalue":"Unrecognized keyword arguments passed to Embedding: {'input_length': 100}","output_type":"error"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense\n\n# Example CNN model\ncnn_model = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n    Conv1D(filters=64, kernel_size=3, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    GlobalMaxPooling1D(),\n    Dense(64, activation='relu'),\n    Dense(num_classes, activation='softmax')\n])\n\n# Example RNN model\nrnn_model = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n    LSTM(units=64, return_sequences=True),\n    LSTM(units=32),\n    Dense(64, activation='relu'),\n    Dense(num_classes, activation='softmax')\n])\n\n# Compile models\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nrnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train models\ncnn_model.fit(train_sequences, train_labels, epochs=10, validation_data=(val_sequences, val_labels))\nrnn_model.fit(train_sequences, train_labels, epochs=10, validation_data=(val_sequences, val_labels))\n\n# Evaluate models\ncnn_loss, cnn_accuracy = cnn_model.evaluate(test_sequences, test_labels)\nrnn_loss, rnn_accuracy = rnn_model.evaluate(test_sequences, test_labels)\n","metadata":{"id":"188TJiVN-XAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n# from sklearn.metrics import accuracy_score\n\n# # Assuming X_train is the TF-IDF transformed data and y_train is the encoded labels\n\n# # Splitting the Data\n# X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# # Define the CNN Model\n# model = Sequential([\n#     Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_split.shape[1], 1)),\n#     MaxPooling1D(pool_size=2),\n#     Flatten(),\n#     Dense(128, activation='relu'),\n#     Dense(len(np.unique(y_train)), activation='softmax')\n# ])\n\n# # Reshape input data to match CNN input shape\n# X_train_split_reshaped = X_train_split.reshape(X_train_split.shape[0], X_train_split.shape[1], 1)\n# X_val_split_reshaped = X_val_split.reshape(X_val_split.shape[0], X_val_split.shape[1], 1)\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# # Training the Model\n# model.fit(X_train_split_reshaped, y_train_split, epochs=10, batch_size=32, validation_data=(X_val_split_reshaped, y_val_split))\n\n# # Evaluate the Model\n# y_pred = model.predict_classes(X_val_split_reshaped)\n# accuracy = accuracy_score(y_val_split, y_pred)\n# print(f'Validation Accuracy: {accuracy}')\n","metadata":{"id":"bCBTLLSVKNNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A multinomial naive Bayes classifier will be created.  I previously did some parameter tuning and found the ngram size of 4 (reflected in the Countvectorizer() instance) and a model alpha of 0.1 did the best.","metadata":{"id":"yPlTJq9L_7j_"}},{"cell_type":"code","source":"### Multinomial Naive Bayes Classifier ###\n# The alpha parameter was determined by grid search previously\n# from sklearn.naive_bayes import MultinomialNB\n# classifier = MultinomialNB(alpha=0.01)\n# classifier.fit(X_train_vectors, y_train)\n# from sklearn.svm import SVC\n\n# classifier = SVC(kernel='linear')\n# classifier.fit(X_train_vectors, y_train)\n# from sklearn.neural_network import MLPClassifier\n\n# # Initialize MLP classifier\n# classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n# classifier.fit(X_train_vectors, y_train)\n\nfrom sklearn.svm import SVC\n\n# Initialize and train the Polynomial Support Vector Classifier (SVC)\nclassifier = SVC(kernel='poly', degree=2)  # Degree can be adjusted\nclassifier.fit(X_train_vectors, y_train)\n\n# from sklearn.svm import SVC\n\n# # Initialize and train the RBF Support Vector Classifier (SVC)\n# classifier = SVC(kernel='rbf')\n# classifier.fit(X_train, y_train)\n\n\n# import xgboost as xgb\n\n# # Initialize and train the Gradient Boosting classifier (XGBoost)\n# classifier = xgb.XGBClassifier()\n# classifier.fit(X_train, y_train)\n\n# from sklearn.ensemble import RandomForestClassifier\n\n# # Initialize and train the Random Forest classifier\n# classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n# classifier.fit(X_train, y_train)\n\n# from sklearn.ensemble import ExtraTreesClassifier\n\n# # Initialize and train the Extra Trees classifier\n# classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n# classifier.fit(X_train, y_train)\n\n# from sklearn.ensemble import BaggingClassifier\n# from sklearn.tree import DecisionTreeClassifier\n\n# # Initialize base decision tree\n# base_classifier = DecisionTreeClassifier()\n\n# # Initialize and train Random Forest with Bagging\n# classifier = BaggingClassifier(base_classifier, n_estimators=100, random_state=42)\n# classifier.fit(X_train, y_train)\n# from sklearn.ensemble import RandomForestClassifier\n\n# # Initialize and train Random Forest classifier with feature subsampling (Random Subspace Method)\n# classifier = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)\n# classifier.fit(X_train, y_train)\n","metadata":{"id":"tJwi906C_7j_","execution":{"iopub.status.busy":"2024-03-27T08:05:38.762265Z","iopub.execute_input":"2024-03-27T08:05:38.762707Z","iopub.status.idle":"2024-03-27T08:05:38.913702Z","shell.execute_reply.started":"2024-03-27T08:05:38.762672Z","shell.execute_reply":"2024-03-27T08:05:38.912825Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"SVC(degree=2, kernel='poly')","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(degree=2, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(degree=2, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test_vectors)","metadata":{"id":"F_Sr9mPk_7kA","execution":{"iopub.status.busy":"2024-03-27T08:05:43.234362Z","iopub.execute_input":"2024-03-27T08:05:43.234789Z","iopub.status.idle":"2024-03-27T08:05:43.254120Z","shell.execute_reply.started":"2024-03-27T08:05:43.234755Z","shell.execute_reply":"2024-03-27T08:05:43.252817Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Okay, so let's look at some model performce metrics like the confusion matrix, accuracy, precision, recall and f1 score.  We are getting really good results on our unseen data, so it looks like our model did not overfit to the training data.  In a real project I would go back and sample many more train test splits since we have a relatively small data set.","metadata":{"id":"A0KJt5or_7kA"}},{"cell_type":"code","source":"X_test_vectors","metadata":{"id":"WT2sNrqkN5Gj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nprint(\"Confusion matrix\\n\")\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\ndef get_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted, average='weighted')\n    recall = recall_score(y_test, y_predicted, average='weighted')\n    f1 = f1_score(y_test, y_predicted, average='weighted')\n    return accuracy, precision, recall, f1\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))","metadata":{"id":"brtrPxfU_7kA","execution":{"iopub.status.busy":"2024-03-27T08:05:47.460456Z","iopub.execute_input":"2024-03-27T08:05:47.460888Z","iopub.status.idle":"2024-03-27T08:05:47.499201Z","shell.execute_reply.started":"2024-03-27T08:05:47.460853Z","shell.execute_reply":"2024-03-27T08:05:47.498050Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Confusion matrix\n\nPredicted    0    1    2    3    4    5\nActual                                 \n0          142    0    2    0    0    0\n1            0  123    0    0    0    0\n2            0    0  927    0    0    0\n3            0    0    0  121    0    0\n4            0    0    0    0  134    0\n5            0    0    0    0    0  409\naccuracy = 0.999 \nprecision = 0.999 \nrecall = 0.999 \nf1 = 0.999\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Assuming you have y_test and y_pred defined earlier\n# y_test = ...\n# y_pred = ...\n\nprint(\"Confusion matrix\\n\")\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\n\ndef get_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted, average='weighted')\n    recall = recall_score(y_test, y_predicted, average='weighted')\n    f1 = f1_score(y_test, y_predicted, average='weighted')\n    return accuracy, precision, recall, f1\n\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))\n\n# Compute ROC curve and ROC area for each class\nlb = LabelBinarizer()\nlb.fit(y_test)\ny_test_bin = lb.transform(y_test)\ny_pred_bin = lb.transform(y_pred)\n\nn_classes = len(lb.classes_)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_bin.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], color='darkorange', lw=2, label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], lw=1, alpha=0.3)  # Plot each class ROC curve with lower opacity\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n","metadata":{"id":"dCJP6-tE_7kB"},"execution_count":null,"outputs":[]}]}